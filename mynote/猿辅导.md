作者：泥锅里的泥鳅
链接：https://www.nowcoder.com/discuss/518890?type=all&order=time&pos=&page=1&channel=1009&source_id=search_all
来源：牛客网


# 1. JVM的内存模型，那几个区容易发生OOM 

内存溢出通俗的讲就是内存不够用了，并且 GC 通过垃圾回收也无法提供更多的内存。实际上除了程序计数器，其他区域都有可能发生 OOM, 简单总结如下：

- 堆内存不足是最常见的 OOM 原因之一，抛出错误信息 `java.lang.OutOfMemoryError:Java heap space`，原因也不尽相同，可能是内存泄漏，也有可能是堆的大小设置不合理。
- 对于虚拟机栈和本地方法栈，导致 OOM 一般为对方法自身不断的递归调用，且没有结束点，导致不断的压栈操作。类似这种情况，JVM 实际会抛出 `StackOverFlowError` , 但是如果 JVM 试图去拓展栈空间的时候，就会抛出 OOM.
- 对于老版的 JDK, 因为永久代大小是有限的，并且 JVM 对老年代的内存回收非常不积极，所以当我们添加新的对象，老年代发生 OOM 的情况也非常常见。
- 随着元数据区的引入，方法区内存已经不再那么窘迫，所以相应的 OOM 有所改观，出现 OOM，异常信息则变成了：“java.lang.OutOfMemoryError: Metaspace”。

# 2. JVM的垃圾回收算法，优点缺点比较 

# 3. JVM中的垃圾回收器(重点介绍CMS,G1) 

# 4. JVM中发生FullGC的情况，old区引用young区对象怎么办？(卡表记录) 

# jvm的卡表（Card Table）

![img](https://csdnimg.cn/release/blogv2/dist/pc/img/reprint.png)

[我叫周利东](https://me.csdn.net/qq_36951116) 2019-07-04 16:21:57 ![img](https://csdnimg.cn/release/blogv2/dist/pc/img/articleReadEyes.png) 2720 ![img](https://csdnimg.cn/release/blogv2/dist/pc/img/tobarCollect.png) 收藏 1

分类专栏： [java](https://blog.csdn.net/qq_36951116/category_7797202.html)

转载自：https://juejin.im/post/5c39920b6fb9a049e82bbf94

 

# JVM之卡表（Card Table）

我们知道，JVM在进行垃圾收集时，需要先标记所有可达对象，然后再清除不可达对象，释放内存空间。那么，如何快速的找到所有可达对象呢？

最简单粗暴的实现，就是每次进行垃圾收集时，都对整个堆中的所有对象进行扫描，找到所有存活对象。逻辑是简单，但性能比较差。

简单粗暴的实现方式，通常都是不可取的。那JVM是如何实现快速标记可达对象的？

答案是GC Roots。

GC Roots是垃圾收集器寻找可达对象的起点，通过这些起始引用，可以快速的遍历出存活对象。GC Roots最常见的是静态引用和堆栈的局部引用变量。然而，这不是我们这讲的重点：)

现代JVM，堆空间通常被划分为新生代和老年代。由于新生代的垃圾收集通常很频繁，如果老年代对象引用了新生代的对象，那么，需要跟踪从老年代到新生代的所有引用，从而避免每次YGC时扫描整个老年代，减少开销。

对于HotSpot JVM，使用了卡标记（Card Marking）技术来解决老年代到新生代的引用问题。具体是，使用卡表（Card Table）和写屏障（Write Barrier）来进行标记并加快对GC Roots的扫描。

## 卡表（Card Table）

基于卡表（Card Table）的设计，通常将堆空间划分为一系列2次幂大小的卡页（Card Page）。

卡表（Card Table），用于标记卡页的状态，每个卡表项对应一个卡页。

HotSpot JVM的卡页（Card Page）大小为512字节，卡表（Card Table）被实现为一个简单的字节数组，即卡表的每个标记项为1个字节。

当对一个对象引用进行写操作时（对象引用改变），写屏障逻辑将会标记对象所在的卡页为dirty。

OpenJDK/Oracle 1.6/1.7/1.8 JVM默认的卡标记简化逻辑如下：

```
CARD_TABLE [this address >> 9] = 0;



复制代码
```

首先，计算对象引用所在卡页的卡表索引号。将地址右移9位，相当于用地址除以512（2的9次方）。可以这么理解，假设卡表卡页的起始地址为0，那么卡表项0、1、2对应的卡页起始地址分别为0、512、1024（卡表项索引号乘以卡页512字节）。

其次，通过卡表索引号，设置对应卡标识为dirty。

 

![img](https://user-gold-cdn.xitu.io/2019/1/12/16840e28f6da1e27?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

 

 

## 带来的2个问题

### 1.无条件写屏障带来的性能开销

每次对引用的更新，无论是否更新了老年代对新生代对象的引用，都会进行一次写屏障操作。显然，这会增加一些额外的开销。但是，与YGC时扫描整个老年代相比较，这个开销就低得多了。

不过，在高并发环境下，写屏障又带来了虚共享（false sharing）问题。

### 2.高并发下虚共享带来的性能开销

在高并发情况下，频繁的写屏障很容易发生虚共享（false sharing），从而带来性能开销。

假设CPU缓存行大小为64字节，由于一个卡表项占1个字节，这意味着，64个卡表项将共享同一个缓存行。

HotSpot每个卡页为512字节，那么一个缓存行将对应64个卡页一共64*512=32KB。

如果不同线程对对象引用的更新操作，恰好位于同一个32KB区域内，这将导致同时更新卡表的同一个缓存行，从而造成缓存行的写回、无效化或者同步操作，间接影响程序性能。

一个简单的解决方案，就是不采用无条件的写屏障，而是先检查卡表标记，只有当该卡表项未被标记过才将其标记为dirty。

这就是JDK 7中引入的解决方法，引入了一个新的JVM参数-XX:+UseCondCardMark，在执行写屏障之前，先简单的做一下判断。如果卡页已被标识过，则不再进行标识。

简单理解如下：

```
if (CARD_TABLE [this address >> 9] != 0)



  CARD_TABLE [this address >> 9] = 0;



复制代码
```

与原来的实现相比，只是简单的增加了一个判断操作。

虽然开启-XX:+UseCondCardMark之后多了一些判断开销，但是却可以避免在高并发情况下可能发生的并发写卡表问题。通过减少并发写操作，进而避免出现虚共享问题（false sharing）。

## 也用于CMS GC

CMS在并发标记阶段，应用线程和GC线程是并发执行的，因此可能产生新的对象或对象关系发生变化，例如：

- 新生代的对象晋升到老年代；
- 直接在老年代分配对象；
- 老年代对象的引用关系发生变更；
- 等等。

对于这些对象，需要重新标记以防止被遗漏。为了提高重新标记的效率，并发标记阶段会把这些发生变化的对象所在的Card标识为Dirty，这样后续阶段就只需要扫描这些Dirty Card的对象，从而避免扫描整个老年代。

# HotSpot JVM GC机制中的——写屏障

![img](https://csdnimg.cn/release/blogv2/dist/pc/img/original.png)

[LittleMagics](https://me.csdn.net/nazeniwaresakini) 2020-05-05 23:48:54 ![img](https://csdnimg.cn/release/blogv2/dist/pc/img/articleReadEyes.png) 702 ![img](https://csdnimg.cn/release/blogv2/dist/pc/img/tobarCollect.png) 收藏 1

版权

### 前言

在比较久之前的一篇文章[《再谈JVM里的记忆集合》](https://www.jianshu.com/p/da5717e5b5ad)中，笔者曾经写了这么一段话：

> HotSpot通过写屏障（write barrier）来维护卡表。我们已经知道，内存屏障的主要作用是防止指令重排序，它也是volatile关键字的基础。有了写屏障，JVM就可以保证引用发生改变时，对卡表中的卡做标记与访问内存的顺序不发生变化。在CardTableRS初始化时，所做的第一件事就是创建卡表需要的屏障集合（barrier set）……

为什么划掉了呢？（原文也已经划掉了）

在上文中，笔者犯了一个原则性的低级错误，即把Java内存模型中的内存屏障/内存栅栏（memory barrier/fence）与HotSpot GC机制中的写屏障（write barrier）混为一谈了，但这两者实际上毫无关系。Doug Lea大佬也在[《JSR-133 Cookbook for Compiler Writers》](https://links.jianshu.com/go?to=http%3A%2F%2Fgee.cs.oswego.edu%2Fdl%2Fjmm%2Fcookbook.html)一文中明确指出：

> Memory barriers are unrelated to the kinds of "write barriers" used in some garbage collectors.

带来困扰，十分抱歉。

写屏障是个有些复杂的话题，准确来说，它在GC中的应用也有不止一处。下面先纠正错误，简单解释写屏障的概念，及其与卡表的关系。

### 写屏障与卡表

“写屏障”这个词虽然看起来高深，但是它的含义却相当naive——就是**对一个对象引用进行写操作（即引用赋值）之前或之后附加执行的逻辑**，相当于为引用赋值挂上的一小段钩子代码。

前文所述“HotSpot通过写屏障来维护卡表”，写屏障就是在将引用赋值写入内存之前，先做一步mark card——即将出现跨代引用的内存块对应的卡页置为dirty，如下图所示。

![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xOTUyMzAtNjQ1ZjEyZmIwNTcyM2YxOC5wbmc?x-oss-process=image/format,png)

而之前提到过的JVM参数`-XX:+UseCondCardMark`，就是开启有条件的写屏障：在将卡页置为dirty之前，先检查它是否已经为dirty状态，如果已经是了，就不必再执行mark card动作，以避免虚共享。

写屏障除了用于维护卡表之外，在并行GC（如CMS、G1）中的并发标记阶段还有一个更重要的用途。下面以CMS垃圾收集器为例简单解说。

### 并行GC中并发标记的漏标隐患

我们已经知道，CMS垃圾收集器的执行分为以下6个阶段：

1. 初始标记
2. 并发标记
3. 并发预清理
4. 重新标记
5. 并发清理
6. 并发重置

其中，只有不带“并发”字眼的初始标记、重新标记两个阶段是stop-the-world的，其他4个阶段都是与用户线程（GC界的术语称作mutator）并行的，这符合CMS收集器追求最少STW时间与最高响应度的宗旨。

初始标记和并发标记阶段就是进行可达性分析。CMS的根搜索机制是深度优先的[三色标记（tri-color marking）算法](https://links.jianshu.com/go?to=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FTracing_garbage_collection%23Tri-color_marking)，属于基础知识，不再展开讲了。

初始标记阶段会只遍历GC Roots直接可达的那些对象，并压入标记栈（mark stack）；并发标记阶段会逐一从标记栈中弹出对象，然后不断递归标记它们直接引用的对象，重复压入-弹出过程，直到标记栈为空。

在并发标记阶段，难点在于：用户线程并未停止，仍然在改变对象的引用关系。这有可能造成原本活动的对象被漏标，进而破坏GC的正确性。示例如下图所示。

![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xOTUyMzAtNDc2NGJiN2RkNjJlMzg0ZC5wbmc?x-oss-process=image/format,png)

(a) 搜索对象A的直接子对象，标记对象B为可达（灰色），并将B压入标记栈。此时A的直接子对象搜索完毕，标记为存活（黑色），将A弹出标记栈；

(b) (c) 与此同时，用户线程改变引用，让A引用C，并移除掉B对C的引用；

(c) 结果：无法再由B标记到C，但也无法由A标记到C（因为A已经出栈）。C虽然仍为活动对象，但被错判为非活动（白色）对象而被回收。显然这是无法容忍的。

事实上，不止是CMS，在其他任何并行的垃圾回收器中，都有对象漏标的隐患。Wilson指出，出现漏标的充要条件是以下两个情况同时发生：

1. mutator使黑色对象直接引用了白色对象；
2. mutator删除了从灰色对象到白色对象之间的所有引用路径。

### 强三色不变式与增量更新写屏障

为了解决漏标问题，需要破坏上文所述的两个情况，亦即强制回收器满足如下两个条件之一。

- **强三色不变式**：保证永远不会存在黑色对象到白色对象的引用（破坏情况1）。
- **弱三色不变式**：所有被黑色对象引用的白色对象都处于灰色保护状态，即直接或间接从灰色对象可达（破坏情况2）。

强/弱三色不变式都可以通过屏障技术来实现，并且在不同环境下有多种不同的屏障技术。CMS收集器采用增量更新（incremental update）写屏障实现强三色不变式，具体来讲，是Dijkstra等人提出的Dijkstra写屏障，其逻辑是：

> **拦截使黑色对象引用指向白色对象的mutate操作，强制被引用指向的白色对象置为灰色状态，并将其压入标记栈。**

Dijkstra写屏障的逻辑用伪码表示如下。

 

```
write_barrier(obj, field, newobj) {



  if(newobj.mark == FALSE) {



    newobj.mark = TRUE



    push(newobj, $mark_stack)



  }



  *field = newobj



}
```

可见，之所以名为“增量更新”，是指写屏障会持续hook引用的插入和变更。下图示出了加入增量更新写屏障后，并发标记阶段引用发生更改的情况，可见对象C可以安全地存活了。

![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xOTUyMzAtZDE5MDFiOTZhNzFmNzE5Ni5wbmc?x-oss-process=image/format,png)

但是，增量更新写屏障无法探知堆外（如栈上）GC Roots的引用变化，所以CMS收集器在并发标记和预清理完成后，还得做一次重新标记，即再做一次根搜索。

### The End

G1的解决思路与CMS又有不同，是采用了初始快照（snapshot-at-the-beginning, SATB）写屏障实现了弱三色不变式。G1垃圾收集器非常复杂，应该择日好好总结一下，今天就不提了。

明天早起搬砖，民那晚安。

# 5. Java线程池(7大参数) 

# 6.       当有任务提交时，线程池的运行原理 

场景：服务端的程序经常会遇到来自客户端传入的短小的任务，执行时间短暂，工作内容单一，并且需要快速的返回结果，如果服务器每次接收一个任务创建一个线程，然后进行执行，这在任务少的时候不错，如果有成千上万的任务递交给服务器，服务器将会创建成千上万的线程，创建成千上万的线程将会带给服务器巨大的压力，大量线程频繁的切换上下文将增加系统的负载和大量资源的开销，同时创建的线程也不方便管理，通过线程池技术可以很好的解决这个问题，预先创建若干线程，并且将线程的创建控制权交给线程池，重复使用线程，减小了服务器的压力，降低了资源的开销和使用，降低了系统的负载，提高相应速度，方便管理线程进行统一的分配调优和监控。
提交一个新的任务到线程池的时候，

## 线程池的处理流程是:

1.线程池首先判断核心线程池中的线程是都在执行任务(核心线程池是否已经满了)，如果不是，创建一个新的工作线程来执行任务，如果都在执行任务则进入下一个流程
2.线程池判断工作队列是否已满，如果工作队列没有满，则将新提交的任务存储在工作队列里，如果工作队列已经满了，则进入写一个流程中
3.线程池判断线程池中的线程是否都处于工作状态(线程池是否已经满了)，如果没有则创建一个新的工作线程来执行任务，如果已经满了则交给包和策略处理这个任务
客户端通过执行execute(job）方法将job提交到线程池 中执行，而客户端自身不用等待job执行完成，线程池中还有增加和减少工作者线程，关闭线程池的方法，工作者线程是一个重复执行job的线程，而每个由客户端提交的job都将进入一个工作队列中等待工作者线程处理

## ThreadPoolExecuteor执行execute()方法分为4种情况

1.如果核心线程池中当前运行的线程少于corePoolSize ，将创建一个新的线程来执行任务(需要获得全局锁)
2.如果核心线程池中当前运行的线程等于或者是多于corePoolSize则将任务加入BlockingQueue
3.如果BlockingQueue队列已经满了，则创建新的线程来处理任务(需要获得全局锁)
4.如果线程池中当前运行的线程超过了最大线程数，则交给饱和策略处理，拒绝执行任务并调用rejectedExecution()方法
客户端将提交一个任务交给线程池的处理流程对应的就是TreadPoolExecutor执行execute()方法的过程
ThreadPoolExecutor采取上述步骤的实际思路是为了在执行execute()方法的时候尽可能的避免获取全局锁。在ThreadPoolExecutor完成预热之后(当前核心线程池中的线程数目大于等于corePoolSzie)几乎所有调用execute()的方法都是在执行将任务添加到BlockingQueue中，因为这样的话不需要获取全局锁。实际上是没有核心线程的，核心线程池是线程池中的一部分，只要判断当前运行的线程的数目是否大于核心线程池的数目就可以。
工作线程:线程池创建线程的时候，会将线程封装称为工作者线程worker,在工作者线程执行完后，还会循环获取工作队列中的任务来执行。线程池中的线程执行任务分为 两种情况，在execute方法中创建一个线程，会让这个线程执行当前任务，这个新城执行完任务之后，会反复的从阻塞队列中获取任务来执行。

# 7.       线程池中的是怎么根据keepalive时间来回收线程的 

因为用到`keepAliveTime`这个变量,整个`ThreadPoolExecutor.java`中只有一个方法,只看这个方法就可以了,这个方法是`getTask()`,其中用到`keepAliveTime`的代码是这样的:

```java
try {
    Runnable r = timed ?
        workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) :
    workQueue.take();
    if (r != null)
        return r;
    timedOut = true;
} catch (InterruptedException retry) {
    timedOut = false;
}
12345678910
```

其中最重要的就是这句
`workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) :`
有什么用呢,看一下`workQueue`是什么:
`private final BlockingQueue<Runnable> workQueue;`
一个阻塞队列,即 核心线程数 =< 当前线程数 && 当前线程数 < 最大线程数时,我们将任务封装放进一个阻塞队列,这就是那个阻塞队列;
然后看这个阻塞队列方法
`E poll(long timeout, TimeUnit unit)`
源码解释是这样的

> Retrieves and removes the head of this queue, waiting up to the specified wait time if necessary for an element to become available.

其实熟悉阻塞队列就应该知道取出元素的方法有两个`poll()`和`take()`,前者是一个非阻塞方法,如果当前队列为空,直接返回,而take()是一个阻塞方法,即如果当前队列为空,阻塞线程,封装线程到AQS的条件变量的条件队列中,而上面的方法是一个介于二者之间的方法,语义是如果队为空,该方法会阻塞线程,但是有一个阻塞时间,如果到时见还没有被唤醒,就自动唤醒;

看到这里就应该知道了,我们的线程在获取任务时,如果队列中已经没有任务,会在此处阻塞`keepALiveTime`的时间,如果到时间都没有任务,就会return null(不是直接返回null,是最终),然后在`runWorker()`方法中,执行
`processWorkerExit(w, completedAbruptly);`
终止线程;

# 8.       怎么来保证线程之间的安全运行 

# 程序多线程运行下怎样保证线程安全

[![img](https://upload.jianshu.io/users/upload_avatars/15717825/65c44fc6-5ef0-4d34-a09a-cc6b3555ede5.jpg?imageMogr2/auto-orient/strip|imageView2/1/w/96/h/96/format/webp)](https://www.jianshu.com/u/f678132fb18b)

[AmbitiousMouse](https://www.jianshu.com/u/f678132fb18b)关注

2019.08.11 20:28:20字数 1,593阅读 1,134

保证线程安全以是否需要同步手段分类，分为同步方案和无需同步方案。 



![img](https://upload-images.jianshu.io/upload_images/15717825-312555a8740be85a.png?imageMogr2/auto-orient/strip|imageView2/2/w/628/format/webp)

**1.互斥同步**

  互斥同步是最常见的一种并发正确性保障手段，同步是指在多线程并发访问共享数据时，保证共享数据在同一时刻只被一个线程使用（同一时刻，只有一个线程操作共享数据），而互斥是实现同步的一个手段，临界区，互斥量和信号量都是主要的互斥实现方式。因此，互斥是因，同步是果；互斥是方法，同步是目的。

在Java中，最基本的互斥同步手段就是synchronized关键字，synchronized关键字在编译后，会在同步块的前后分别形成monitorenter和monitorexit这两个字节码质量，这两个字节码指令都需要一个reference类型的参数来指明要锁定和解锁的对象。

此外，ReentrantLock也是通过互斥来实现同步的，在基本用法上，ReentrantLock与synchronized很相似，他们都具备一样的线程重入特性。

互斥同步最主要的问题就是线程阻塞和唤醒所带来的性能问题，因此这种同步也成为阻塞同步。互斥同步属于一种悲观的并发策略，总是认为只要不去做正确地同步措施（例如加锁），那就肯定会出现问题，无论共享数据是否真的会出现竞争，它都要进行加锁。

 **2、非阻塞同步** 

​    随着硬件指令集的发展，出现了基于冲突检测的乐观并发策略，通俗地说，就是先进行操作，如果没有其他线程争用共享数据，那操作就成功了；如果共享数据有争用，产生了冲突，那就再采用其他的补偿措施。（最常见的补偿错误就是不断地重试，直到成功为止），这种乐观的并发策略的许多实现都不需要把线程挂起，因此这种同步操作称为非阻塞同步。

​    非阻塞的实现CAS（compareandswap）：CAS指令需要有3个操作数，分别是内存地址（在java中理解为变量的内存地址，用V表示）、旧的预期值（用A表示）和新值（用B表示）。CAS指令执行时，CAS指令指令时，当且仅当V处的值符合旧预期值A时，处理器用B更新V处的值，否则它就不执行更新，但是无论是否更新了V处的值，都会返回V的旧值，上述的处理过程是一个原子操作。

​    CAS缺点：

​    ABA问题：因为CAS需要在操作值的时候检查下值有没有发生变化，如果没有发生变化则更新，但是一个值原来是A，变成了B，又变成了A，那么使用CAS进行检查时会发现它的值没有发生变化，但是实际上却变化了。

​    ABA问题的解决思路就是使用版本号或者带上时间戳。在变量前面追加版本号，每次变量更新的时候把版本号加一，那么A-B-A就变成了1A-2B-3C。JDK的atomic包里提供了一个类AtomicStampedReference来解决ABA问题。这个类的compareAndSet方法作用是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。

**3、无需同步方案**

​    要保证线程安全，并不是一定就要进行同步，两者没有因果关系。同步只是保证共享数据争用时的正确性的手段，如果一个方法本来就不涉及共享数据，那它自然就无需任何同步操作去保证正确性，因此会有一些代码天生就是线程安全的。

​    1）可重入代码

​    可重入代码（ReentrantCode）也称为纯代码（Pure Code），可以在代码执行的任何时刻中断它，转而去执行另外一段代码，而在控制权返回后，原来的程序不会出现任何错误。所有的可重入代码都是线程安全的，但是并非所有的线程安全的代码都是可重入的。

​    可重入代码的特点是不依赖存储在堆上的数据和公用的系统资源、用到的状态量都是由参数中传入、不调用 非可重入的方法等。

​    （类比：synchronized拥有锁重入的功能，也就是在使用synchronized时，当一个线程得到一个对象锁后，再次请求此对象锁时时可以再次得到该对象的锁）

   2）线程本地存储

​    如果一段代码中所需的数据必须与其他代码共享，那就看看这些共享数据的代码是否能保证在同一个线程中执行。如果能保证，我们就可以把共享数据的可见范围限制在同一个线程之内。这样无需同步也能保证线程之间不出现数据的争用问题。

​    符合这种特点的应用并不少见，大部分使用消费队列的架构模式（如“生产者-消费者”模式）都会将产品的消费过程尽量在一个线程中消费完。其中最重要的一个应用实例就是经典的Web交互模型中的“一个请求对应一个服务器线程（Thread-per-Request）”的处理方式，这种处理方式的广泛应用使得很多Web服务器应用都可以使用线程本地存储来解决线程安全问题。

# 9.Java中实现锁的方式(显示锁和Synchronized的区别) 

# 显示锁和synchronized的区别

![img](https://csdnimg.cn/release/blogv2/dist/pc/img/original.png)

[_xuzhi_](https://me.csdn.net/weixin_42247563) 2019-10-08 09:54:33 ![img](https://csdnimg.cn/release/blogv2/dist/pc/img/articleReadEyes.png) 40 ![img](https://csdnimg.cn/release/blogv2/dist/pc/img/tobarCollect.png) 收藏

分类专栏： [java](https://blog.csdn.net/weixin_42247563/category_8635542.html) [并发](https://blog.csdn.net/weixin_42247563/category_9226097.html) [java基础](https://blog.csdn.net/weixin_42247563/category_9229500.html)

版权

​    这两种锁都是可重入锁。synchronized内置锁是一种非公平平锁排它锁，是jdk1.8对它做了大量的优化（偏向锁->轻量级锁->重量级锁），所以在没有特殊用途的情况下建议使用synchronized关键字(java语言内置的，加锁释放锁都是已经固化好的我们不需要处理，显示锁是语法层面的锁，有一个固定的范式)。

​    显示锁有一个固定的结构jdk源码给出的示例如下：

class X {
    private final ReentrantLock lock = new ReentrantLock();
    // ...
   public void m() {
      lock.lock();
      try {
         // ... method body
       } finally {
          lock.unlock()
       }
    }
 }

即显示锁搭配try{....}finally{..} 语句使用确保锁，不管发生什么情况都能关闭，

注：当一个线程结束时，所持与的所有资源，锁都会释放掉。显示锁除了支持可重入外，还支持超时拿锁，响应中断等。

公平锁就绪线程按照公平策略（按请求顺序）依次等待获得锁，效率低

非公平锁无视等待队列，当锁处于闲置时就竞争获得锁，会出现插队的现象，效率高（线程的上线文切换会有一定的时间消耗，而非公平锁，可以充分的利用这些时间）

10. Synchronized底层的原理(锁池和等待池) 
11. JUC中的ReentrantLock底层(AQS) 
12. TCP的可靠连接怎么保证 
13. TCP和UDP的区别 

# 数据库的底层的B+数结构，为什么要使用B+，而不是AVL 

## 当你回答使用B+ 怎么怎么好的时候，其实这道面试题你就注定答不满分了，你应该是从一步步如果演变到使用B+来做MySQL的数据结构，下面就一步一步从二叉树——>AVL(平衡二叉树)——>B Tree（多路平衡查找树）——>B+ Tree的一个演变的过程来进行分析，为什么使用B+ Tree的？

**（1）先从二叉树开始说起：**

- 首先你得知道二叉树是什么吧：看下面的图一你就该很熟悉了吧
- 然后你得知道二叉树查询的时间复杂度是O(log2(n))，这样感觉其实二叉树的查询效率挺高的，但是他会出现另一种现象，就是下面的图二：
- 这样就导致了二叉树的查询效率不问题，如果运气好的话查询效率就很高，如果运气不好的话，就会出现图二的情况，因此在二叉树的基础上又进行的改进，演变出来了平衡二叉树（AVL树）

图一：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190630142207259.png)
图二：
![在这里插入图片描述](https://img-blog.csdnimg.cn/2019063014250816.png)

**（2）然后到了平衡二叉树（AVL树）:**

1. 首先你得知道平衡二叉树的定义吧：在满足二叉树的基础上，任意两个节点的两个子树的高度差不能超过1：就好比下面的这个图的一个效率很差的二叉树，比如5节点的左子树的高度是0，右字树的高度是2，，所以很明显不满足平衡二叉树的概念
   ![在这里插入图片描述](https://img-blog.csdnimg.cn/20190630143759406.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NTIwMjM1,size_16,color_FFFFFF,t_70)
2. AVL树主要是为了解决上面的图出现的情况，所以现在要把上图的二叉树转插入一个“9”节点然后换为一个平衡二叉树的情况就是下面的：
   ![在这里插入图片描述](https://img-blog.csdnimg.cn/20190630144830778.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NTIwMjM1,size_16,color_FFFFFF,t_70)
3. 可以看出平衡二叉树的缺点就是：（1）维护平衡过程的成本代价很高，因为每次删除一个节点或者增加一个节点的话，需要一次或者多次的左旋，右旋等去维护“平衡”状态，（2）然后是查询的效率不稳定，还是会有看运气的成分在里面，（3）然后是如果节点很多的话，那么这个AVL树的高度还是会很高的，那么查询效率还是会很低，
4. 还有就是节点存储的数据内容太少。没有很好利用操作系统和磁盘数据交换特性，也没有利用好磁盘IO的预读能力。因为操作系统和磁盘之间一次数据交换是已页为单位的，一页 = 4K，即每次IO操作系统会将4K数据加载进内存。但是，在二叉树每个节点的结构只保存一个关键字，一个数据区，两个子节点的引用，并不能够填满4K的内容。幸幸苦苦做了一次的IO操作，却只加载了一个关键字，在树的高度很高，恰好又搜索的关键字位于叶子节点或者支节点的时候，取一个关键字要做很多次的IO。因此平衡二叉树也是不太符合MySQL的查询结构的。

**（3）然后到了使用B Tree（多路平衡查找树）**

1. 首先你也得知道B Tree的基本概念：所有的叶子节点的高度都是一样，这个保证了每次查询数据的时候都是稳定的查询效率，不会因为运气的影响
2. 然后B Tree中其实每个非叶子节点内的小节点内其实都是一个二元组[key, data]，key其实就是下图的那个25这种的，然后这个data其实对应的就是数据库中id等于25这条完整的数据记录的内存地址（因为在Myisam中他是数据和索引数据是分开的）

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190630162502592.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NTIwMjM1,size_16,color_FFFFFF,t_70)
**B树的特点：**

1. 首先B Tree的每一个节点上其实是有date的，这个date其实就是
2. 然后是B Tree查询的效率不够稳定，他有可能在第一个节点中就查到了数据，并且返回
3. 他的键值其实都是分布在整棵树上的节点上的任何一个节点

**（4）然后到了使用B+ Tree（多路平衡查找树）**

1. 首先你要知道什么B+ Tree，其实他是专门为磁盘或者其他的直接存取辅助设备设计的一种平衡查找树，在B树中，所有的节点都是按照键值的大小顺序存放在同一层的叶子节点上，由各叶子节点的指针连接。
2. 下图的一颗B树，是一个高度为2，每一页可以放4条记录，扇出是5。
   ![在这里插入图片描述](https://img-blog.csdnimg.cn/20190630172014645.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NTIwMjM1,size_16,color_FFFFFF,t_70)
   **重要的第一点：**
3. 重要的第一点：B+ Tree有一个很大的改变就是他的每一个非叶子节点的内节点中都没有date这个概念了，都变成了key，因为他的date都放在了叶子节点上，这样的一个最大的好处就利用了局部性原理（当一个数据被用到时，其附近的数据也通常会马上被使用）与磁盘预读的特性（磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据【这个一定的长度就是一个节点的大小设置为16K】放入内存）
4. 接着上面的：预读的长度一般为页（page）的整倍数。页是计算机管理存储器的逻辑块，硬件及操作系统往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（在许多操作系统中，页得大小通常为4k），主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后异常返回，程序继续运行。
5. 重要的第二点：由于上面我们说的预读原理，因为B+ Tree中节点的内节点无 data 域，其实就是因为没有date域了，但是每次IO的页的大小是固定的，但是B+Tree中没有了date域，那么肯定每次IO读取若干个块块中包含的Key域的值肯定更多啊，B+树单次磁盘 IO 的信息量大于B树，从这点来看B+树相对B树磁盘 IO 次数少。
6. 数据库系统的设计者巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入。
7. 为了达到这个目的在实际实现B-Tree还需要使用如下技巧：每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了一个node只需一次I/O。

**重要的第二点：**

1. B+Tree中因为数据都在叶子节点，所以每次查询的时间复杂度是固定的，因为稳定性保证了
2. 而且叶子节点之间都是链表的结构，所以B+ Tree也是可以支持范围查询的，而B树每个节点 key 和 data 在一起，则无法区间查找。

# Mysql的4个隔离级别，对应解决了哪些问题 

# 隔离级别中RR问题是怎么解决的 

# 数据库中的MVCC实现机制 

# 数据库中怎么解决幻读问题的—间隙锁 

# HashMap的扩容机制。加入都1亿个数据，是一次性rehash完么？ 

# 渐进式的rehash，实现原理 

1. 算法题 ---DP找矩阵中的最小路径 

猿辅导二面(8.14) 
# 双亲委派机制。有哪些好处。怎么打破。JAVA中哪些东西打破了双亲委派机制。 

上下文线程类加载器是怎么打破的。上下文类加载器底层工作原理是什么？ 

2. Java可以自己重新写lang包下的类么？ 
3.       线程池中的线程的状态。状态之间的转换关系 
4. Yield()方法有什么用 
5.       为什么要废弃suspend和stop方法 
6.       怎么去停止一个正在运行中的线程 
7.       线程池中为什么要使用workqueue？ 
8. Java中的直接内存 
9. Java中的NIO和BIO 
10. NIO底层实现(分析三种实现方式) 
11. CopyOnwriteArraylist和ConcurrentLinkedList底层实现 
12. AQS底层 
13. Redis中的持久化机制 
14. Redis的集群说一说 
15.   一致性Hash的好处与缺点(偏环) 
16.   算法题 判断一个字符串是否是累加和字符串: 
eg:”11235813”是一个累加和字符串 
eg:”10099199” 100 + 99  = 199 也是一个累加和字符串 


猿辅导三面(8.25) 
1.       聊比赛，聊项目 
2. Java中的NIO和BIO区别 
3. Java中的直接内存 
4.       直接内存产生OOM怎么办？ 
5.       项目中有遇到这种情况么？怎么解决的 
6. Redis中RDB持久化的具体过程 
7.       操作系统的进程通信方法 
8. JVM中各个区具体存放哪些数据 
9.       项目中遇到了哪些问题 

10. 算法题 : 多个有序链表合并成一个有序的链表

# 从场景上说明为什么要使用线程池，线程池处理提交任务的流程是什么，线程池的的运行原理是什么

![img](https://csdnimg.cn/release/blogv2/dist/pc/img/original.png)

[一只小猛子](https://me.csdn.net/qq_41174684) 2019-05-15 17:17:32 ![img](https://csdnimg.cn/release/blogv2/dist/pc/img/articleReadEyes.png) 683 ![img](https://csdnimg.cn/release/blogv2/dist/pc/img/tobarCollect.png) 收藏

版权

场景：服务端的程序经常会遇到来自客户端传入的短小的任务，执行时间短暂，工作内容单一，并且需要快速的返回结果，如果服务器每次接收一个任务创建一个线程，然后进行执行，这在任务少的时候不错，如果有成千上万的任务递交给服务器，服务器将会创建成千上万的线程，创建成千上万的线程将会带给服务器巨大的压力，大量线程频繁的切换上下文将增加系统的负载和大量资源的开销，同时创建的线程也不方便管理，通过线程池技术可以很好的解决这个问题，预先创建若干线程，并且将线程的创建控制权交给线程池，重复使用线程，减小了服务器的压力，降低了资源的开销和使用，降低了系统的负载，提高相应速度，方便管理线程进行统一的分配调优和监控。
提交一个新的任务到线程池的时候，

## 线程池的处理流程是:

1.线程池首先判断核心线程池中的线程是都在执行任务(核心线程池是否已经满了)，如果不是，创建一个新的工作线程来执行任务，如果都在执行任务则进入下一个流程
2.线程池判断工作队列是否已满，如果工作队列没有满，则将新提交的任务存储在工作队列里，如果工作队列已经满了，则进入写一个流程中
3.线程池判断线程池中的线程是否都处于工作状态(线程池是否已经满了)，如果没有则创建一个新的工作线程来执行任务，如果已经满了则交给包和策略处理这个任务
客户端通过执行execute(job）方法将job提交到线程池 中执行，而客户端自身不用等待job执行完成，线程池中还有增加和减少工作者线程，关闭线程池的方法，工作者线程是一个重复执行job的线程，而每个由客户端提交的job都将进入一个工作队列中等待工作者线程处理

## ThreadPoolExecuteor执行execute()方法分为4种情况

1.如果核心线程池中当前运行的线程少于corePoolSize ，将创建一个新的线程来执行任务(需要获得全局锁)
2.如果核心线程池中当前运行的线程等于或者是多于corePoolSize则将任务加入BlockingQueue
3.如果BlockingQueue队列已经满了，则创建新的线程来处理任务(需要获得全局锁)
4.如果线程池中当前运行的线程超过了最大线程数，则交给饱和策略处理，拒绝执行任务并调用rejectedExecution()方法
客户端将提交一个任务交给线程池的处理流程对应的就是TreadPoolExecutor执行execute()方法的过程
ThreadPoolExecutor采取上述步骤的实际思路是为了在执行execute()方法的时候尽可能的避免获取全局锁。在ThreadPoolExecutor完成预热之后(当前核心线程池中的线程数目大于等于corePoolSzie)几乎所有调用execute()的方法都是在执行将任务添加到BlockingQueue中，因为这样的话不需要获取全局锁。实际上是没有核心线程的，核心线程池是线程池中的一部分，只要判断当前运行的线程的数目是否大于核心线程池的数目就可以。
工作线程:线程池创建线程的时候，会将线程封装称为工作者线程worker,在工作者线程执行完后，还会循环获取工作队列中的任务来执行。线程池中的线程执行任务分为 两种情况，在execute方法中创建一个线程，会让这个线程执行当前任务，这个新城执行完任务之后，会反复的从阻塞队列中获取任务来执行。